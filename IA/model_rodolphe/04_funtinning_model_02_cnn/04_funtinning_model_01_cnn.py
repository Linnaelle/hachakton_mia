# -*- coding: utf-8 -*-
"""04_funtinning_model_01_cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VKpO94rsl782MjrQFdDsGf8aTNWd6cyh

# Package install
"""

!pip install mtcnn
!pip install lz4

!pip show tensorflow

"""# Import library"""

import tensorflow as tf
import numpy as np
import os
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization, Flatten

"""# Verification GPU Active"""

import torch
torch.cuda.is_available()

import tensorflow as tf

# Liste des dispositifs GPU détectés
gpus = tf.config.list_physical_devices('GPU')
print("GPU disponibles :", gpus)

# Affiche le nom du GPU utilisé (si disponible)
print("GPU en cours d'utilisation :", tf.test.gpu_device_name())

!nvidia-smi

"""# Preparation des données"""

import zipfile
import os

# Chemin vers le fichier zip
zip_path = 'affectnet_processed_traitement_v1.zip'

# Dossier de destination
extract_to = 'dataset_2'

# Création du dossier de destination s'il n'existe pas
if not os.path.exists(extract_to):
    os.makedirs(extract_to)

# Extraction du contenu du fichier zip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print(f"Extraction terminée dans le dossier {extract_to}.")

# -----------------------------------------------------------------------------
# 2. Dictionnaire des émotions basé sur AffectNet
# -----------------------------------------------------------------------------
emotion_dict = {
    0: "neutral",
    1: "happy",
    2: "sad",
    3: "surprise",
    4: "fear",
    5: "disgust",
    6: "angry"
}

# -----------------------------------------------------------------------------
# 3. Définition des chemins vers le nouveau dataset
# -----------------------------------------------------------------------------
# La structure du dataset_2 est la suivante :
# dataset_2/
# ├── train/
# │   ├── 0
# │   ├── 1
# │   ├── 2
# │   ├── 3
# │   ├── 4
# │   ├── 5
# │   └── 6
# └── val/
#     ├── 0
#     ├── 1
#     ├── 2
#     ├── 3
#     ├── 4
#     ├── 5
#     └── 6
new_train_dir = '/content/dataset_2/train'
new_val_dir   = '/content/dataset_2/val'

# -----------------------------------------------------------------------------
# 4. Création de DataFrames pour visualiser une image par classe
# -----------------------------------------------------------------------------
def create_df_from_directory(directory):
    data = []
    for folder in sorted(os.listdir(directory)):
        folder_path = os.path.join(directory, folder)
        if os.path.isdir(folder_path):
            img_files = os.listdir(folder_path)
            if len(img_files) > 0:
                first_img = os.path.join(folder_path, img_files[0])
                data.append({"img_path": first_img, "label": int(folder)})
    return pd.DataFrame(data)

df_train = create_df_from_directory(new_train_dir)
df_val   = create_df_from_directory(new_val_dir)

# -----------------------------------------------------------------------------
# 5. Fonction d'affichage d'une image avec son label et ses dimensions
# -----------------------------------------------------------------------------
def afficher_image(df, index):
    chemin_image = df.loc[index, 'img_path']
    label = df.loc[index, 'label']
    image = cv2.imread(chemin_image)
    if image is None:
        print(f"Image introuvable : {chemin_image}")
        return
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    dims = image.shape  # (hauteur, largeur, canaux)
    plt.imshow(image_rgb)
    plt.title(f"Émotion : {emotion_dict.get(label, 'Inconnu')}\nDims: {dims}")
    plt.axis('off')
    plt.show()

# Affichage d'exemples d'images pour chaque classe dans l'ensemble d'entraînement
print("Exemples d'images dans l'ensemble d'entraînement :")
for lab in df_train['label'].unique():
    idx = df_train[df_train['label'] == lab].index[0]
    afficher_image(df_train, idx)

# Affichage d'exemples d'images pour chaque classe dans l'ensemble de validation
print("Exemples d'images dans l'ensemble de validation :")
for lab in df_val['label'].unique():
    idx = df_val[df_val['label'] == lab].index[0]
    afficher_image(df_val, idx)

"""## Training approchement 1 -> Fine tunning"""

# -----------------------------------------------------------------------------
# Pour l'entraînement : augmentation légère + normalisation
train_datagen = ImageDataGenerator(
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    rescale=1./255
)

# Pour la validation : seule la normalisation
val_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=new_train_dir,
    target_size=(48, 48),
    batch_size=64,
    color_mode="grayscale",
    class_mode="categorical"
)

val_generator = val_datagen.flow_from_directory(
    directory=new_val_dir,
    target_size=(48, 48),
    batch_size=64,
    color_mode="grayscale",
    class_mode="categorical"
)

# -----------------------------------------------------------------------------
# 8. Construction du nouveau modèle en ajoutant de nouvelles couches
# -----------------------------------------------------------------------------
# Chargement de l'ancien modèle
old_model = load_model('best_model.h5')
old_model.summary()

# Créer un tenseur d'entrée explicite (correspondant à la forme de vos images)
new_input = Input(shape=(48, 48, 1))

# Passer ce nouvel input à travers toutes les couches de l'ancien modèle sauf la dernière
x = new_input
for layer in old_model.layers[:-1]:
    x = layer(x)
base_output = x

# Geler les couches de l'ancien modèle pour conserver leurs poids
for layer in old_model.layers:
    layer.trainable = False

# Ajout de nouvelles couches supplémentaires avec des noms uniques pour éviter les conflits
x = Dense(256, activation='relu', name='new_dense')(base_output)
x = BatchNormalization(name='new_batch_norm')(x)
x = Dropout(0.5, name='new_dropout')(x)
new_output = Dense(7, activation='softmax', name='new_output')(x)

# Création du nouveau modèle en spécifiant le nouvel input et la nouvelle sortie
new_model = Model(new_input, new_output)
new_model.summary()

# -----------------------------------------------------------------------------
# 9. Compilation du nouveau modèle
# -----------------------------------------------------------------------------
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
new_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# -----------------------------------------------------------------------------
# 10. Définition des callbacks pour l'entraînement
# -----------------------------------------------------------------------------
callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        filepath='best_model_updated.h5',  # Chemin de sauvegarde du meilleur modèle
        monitor='val_accuracy',            # Surveillance de l'exactitude sur le set de validation
        save_best_only=True,               # Sauvegarder uniquement le modèle avec la meilleure performance
        mode='max',
        verbose=1
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',                # Arrêt anticipé en cas de stagnation de la perte sur le set de validation
        patience=10,                       # Nombre d'époques d'attente avant arrêt
        restore_best_weights=True,
        verbose=1
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',                # Réduction du learning rate si la perte de validation ne s'améliore pas
        factor=0.5,
        patience=5,
        verbose=1
    )
]

# -----------------------------------------------------------------------------
# 11. Entraînement du nouveau modèle sur le nouveau dataset
# -----------------------------------------------------------------------------
history = new_model.fit(
    train_generator,
    epochs=50,  # Vous pouvez ajuster le nombre d'époques
    validation_data=val_generator,
    callbacks=callbacks
)

# -----------------------------------------------------------------------------
# 12. Visualisation des courbes d'entraînement (accuracy et loss)
# -----------------------------------------------------------------------------
plt.figure(figsize=(12, 5))

# Courbe d'exactitude
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.xlabel('Époque')
plt.ylabel('Exactitude')
plt.legend()
plt.title("Courbe d'Exactitude")

# Courbe de la perte
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.xlabel('Époque')
plt.ylabel('Perte')
plt.legend()
plt.title("Courbe de Perte")
plt.show()

# -----------------------------------------------------------------------------
# 13. Sauvegarde finale du nouveau modèle entraîné
# -----------------------------------------------------------------------------
new_model.save('final_model_updated.h5')



"""## Training approchement 2
2 niveau du fine tunning
"""

import tensorflow as tf
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os

# Vérification de l'utilisation du GPU
print("GPU en cours d'utilisation :", tf.test.gpu_device_name())

# -----------------------------------------------------------------------------
# Phase 0 : Chargement de l'ancien modèle
# -----------------------------------------------------------------------------
old_model = load_model('best_model.h5')
old_model.summary()

# -----------------------------------------------------------------------------
# Phase 1 : Construction du nouveau modèle avec la base gelée
# -----------------------------------------------------------------------------
# On crée un nouvel input explicite
new_input = Input(shape=(48, 48, 1))

# Passage de new_input à travers toutes les couches de l'ancien modèle sauf la dernière
x = new_input
for layer in old_model.layers[:-1]:
    layer.trainable = False  # Gel de toute la base
    x = layer(x)
base_output = x

# Ajout de nouvelles couches sur la base extraite
x = Dense(256, activation='relu', name='new_dense')(base_output)
x = BatchNormalization(name='new_batch_norm')(x)
x = Dropout(0.5, name='new_dropout')(x)
# IMPORTANT : On ajuste le nombre de neurones de sortie à 8 pour le nouveau dataset
new_output = Dense(8, activation='softmax', name='new_output')(x)

# Création du nouveau modèle
new_model = Model(new_input, new_output)
new_model.summary()

# -----------------------------------------------------------------------------
# Compilation du modèle pour la Phase 1
# -----------------------------------------------------------------------------
optimizer_phase1 = tf.keras.optimizers.Adam(learning_rate=1e-4)
new_model.compile(optimizer=optimizer_phase1, loss='categorical_crossentropy', metrics=['accuracy'])

# -----------------------------------------------------------------------------
# Préparation des générateurs d'images (avec augmentation)
# -----------------------------------------------------------------------------
train_dir = '/content/dataset_2/train'
val_dir = '/content/dataset_2/val'

# Générateur pour l'entraînement avec augmentation
train_datagen = ImageDataGenerator(
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    rescale=1./255
)

# Générateur pour la validation (seulement normalisation)
val_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=train_dir,
    target_size=(48, 48),
    batch_size=64,
    color_mode='grayscale',
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_directory(
    directory=val_dir,
    target_size=(48, 48),
    batch_size=64,
    color_mode='grayscale',
    class_mode='categorical'
)

# -----------------------------------------------------------------------------
# Callbacks pour la Phase 1
# -----------------------------------------------------------------------------
callbacks_phase1 = [
    ModelCheckpoint(filepath='best_model_phase1.keras', monitor='val_accuracy', save_best_only=True, verbose=1),
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),
    TensorBoard(log_dir='./logs_phase1', histogram_freq=1),
    CSVLogger('training_phase1.csv', append=True)
]

# -----------------------------------------------------------------------------
# Entraînement - Phase 1 : Entraîner les nouvelles couches avec la base gelée
# -----------------------------------------------------------------------------
print("Phase 1 : Entraînement avec la base gelée")
history_phase1 = new_model.fit(
    train_generator,
    epochs=10,  # Vous pouvez ajuster le nombre d'époques
    validation_data=val_generator,
    callbacks=callbacks_phase1
)

# -----------------------------------------------------------------------------
# Phase 2 : Fine-tuning en débloquant partiellement la base
# -----------------------------------------------------------------------------
# Ici, nous choisissons de débloquer les 4 dernières couches de l'ancien modèle pour permettre un ajustement plus fin.
for layer in old_model.layers[-4:]:
    layer.trainable = True

# Recompilez le modèle avec un taux d'apprentissage plus faible pour le fine-tuning
optimizer_phase2 = tf.keras.optimizers.Adam(learning_rate=1e-5)
new_model.compile(optimizer=optimizer_phase2, loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks pour la Phase 2
callbacks_phase2 = [
    ModelCheckpoint(filepath='best_model_phase2.keras', monitor='val_accuracy', save_best_only=True, verbose=1),
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),
    TensorBoard(log_dir='./logs_phase2', histogram_freq=1),
    CSVLogger('training_phase2.csv', append=True)
]

# -----------------------------------------------------------------------------
# Entraînement - Phase 2 : Fine-tuning avec une partie de la base débloquée
# -----------------------------------------------------------------------------
print("Phase 2 : Fine-tuning avec les dernières couches débloquées")
history_phase2 = new_model.fit(
    train_generator,
    epochs=20,  # Vous pouvez augmenter ce nombre pour un fine-tuning plus poussé
    validation_data=val_generator,
    callbacks=callbacks_phase2
)

# -----------------------------------------------------------------------------
# Sauvegarde finale du modèle
# -----------------------------------------------------------------------------
new_model.save('final_model_updated.keras')

# -----------------------------------------------------------------------------
# 13. Visualisation des courbes d'entraînement (accuracy et loss)
# -----------------------------------------------------------------------------
plt.figure(figsize=(12, 5))

# Courbe d'exactitude
plt.subplot(1, 2, 1)
plt.plot(history_phase2.history['accuracy'], label='Train')
plt.plot(history_phase2.history['val_accuracy'], label='Validation')
plt.xlabel('Époque')
plt.ylabel('Exactitude')
plt.legend()
plt.title("Courbe d'Exactitude")

# Courbe de la perte
plt.subplot(1, 2, 2)
plt.plot(history_phase2.history['loss'], label='Train')
plt.plot(history_phase2.history['val_loss'], label='Validation')
plt.xlabel('Époque')
plt.ylabel('Perte')
plt.legend()
plt.title("Courbe de Perte")
plt.show()







"""# Construction of the dataset for training model 05"""

def afficher_arborescence(dossier, niveau=0, profondeur_max=2):
    """ Affiche l'arborescence des sous-dossiers jusqu'à profondeur_max, sans afficher les fichiers """
    if not os.path.exists(dossier):
        print("Le dossier spécifié n'existe pas.")
        return

    items = sorted(os.listdir(dossier))
    indent = "│   " * niveau

    for i, item in enumerate(items):
        chemin = os.path.join(dossier, item)
        est_dernier = (i == len(items) - 1)
        prefixe = "└── " if est_dernier else "├── "

        # Afficher uniquement les dossiers
        if os.path.isdir(chemin):
            print(indent + prefixe + item)

            # Limite la profondeur d'affichage des sous-dossiers
            if niveau < profondeur_max:
                afficher_arborescence(chemin, niveau + 1, profondeur_max)

# Spécifier le chemin du dossier à afficher
chemin_dossier = "dataset_2/"
afficher_arborescence(chemin_dossier)

import os
import shutil

def supprimer_dossiers_indesirables(racine, dossiers_a_supprimer=["__MACOSX", ".ipynb_checkpoints"]):
    """
    Parcourt récursivement le dossier racine et supprime les dossiers dont le nom figure dans dossiers_a_supprimer.
    """
    # Parcours de l'arborescence en partant du bas (pour éviter des problèmes lors de la suppression)
    for dossier, sous_dossiers, _ in os.walk(racine, topdown=False):
        for sd in sous_dossiers:
            if sd in dossiers_a_supprimer:
                chemin = os.path.join(dossier, sd)
                try:
                    shutil.rmtree(chemin)
                    print(f"Suppression du dossier : {chemin}")
                except Exception as e:
                    print(f"Erreur lors de la suppression de {chemin} : {e}")

# Exemple d'utilisation
chemin_dossier = "dataset_2/"
supprimer_dossiers_indesirables(chemin_dossier)

import shutil

# Chemin du dossier à zipper
dossier = "dataset_2"

# Nom de l'archive (sans l'extension)
nom_archive = "dataset_2_archive"

# Création de l'archive zip
shutil.make_archive(nom_archive, 'zip', dossier)

print(f"Archive {nom_archive}.zip créée avec succès.")





# --- Téléchargement et décompression du dataset ---
#!rm -rf /content/FER2013
#!kaggle datasets download msambare/fer2013 -p /content/FER2013
!unzip -q /content/FER2013/fer2013.zip -d /content/FER2013

# Spécifier le chemin du dossier à afficher
chemin_dossier = "FER2013/"
afficher_arborescence(chemin_dossier)

# Spécifier le chemin du dossier à afficher
chemin_dossier = "dataset_2/"
afficher_arborescence(chemin_dossier)

import os
import shutil

# Définir le nouveau dossier pour fusionner les datasets
nouveau_dataset = "nouveau_dataset"
os.makedirs(nouveau_dataset, exist_ok=True)

# Créer les sous-dossiers train et test dans le nouveau dataset
train_nouveau = os.path.join(nouveau_dataset, "train")
test_nouveau = os.path.join(nouveau_dataset, "test")
os.makedirs(train_nouveau, exist_ok=True)
os.makedirs(test_nouveau, exist_ok=True)

# Liste des catégories
categories = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"]

# Créer les dossiers pour chaque catégorie dans train et test
for cat in categories:
    os.makedirs(os.path.join(train_nouveau, cat), exist_ok=True)
    os.makedirs(os.path.join(test_nouveau, cat), exist_ok=True)

def copier_images(src_dossier, dst_dossier):
    """
    Copie les fichiers du dossier source dans le dossier destination.
    En cas de nom de fichier en doublon, un suffixe est ajouté.
    """
    for filename in os.listdir(src_dossier):
        chemin_src = os.path.join(src_dossier, filename)
        if os.path.isfile(chemin_src):
            chemin_dst = os.path.join(dst_dossier, filename)
            # Gérer les doublons en ajoutant un suffixe
            base, ext = os.path.splitext(filename)
            compteur = 1
            while os.path.exists(chemin_dst):
                chemin_dst = os.path.join(dst_dossier, f"{base}_{compteur}{ext}")
                compteur += 1
            shutil.copy(chemin_src, chemin_dst)

# Fusionner le dataset FER2013
fer2013 = "FER2013"

# Copier les images de FER2013/train dans nouveau_dataset/train
for cat in categories:
    dossier_src = os.path.join(fer2013, "train", cat)
    dossier_dst = os.path.join(train_nouveau, cat)
    if os.path.exists(dossier_src):
        copier_images(dossier_src, dossier_dst)

# Copier les images de FER2013/test dans nouveau_dataset/test
for cat in categories:
    dossier_src = os.path.join(fer2013, "test", cat)
    dossier_dst = os.path.join(test_nouveau, cat)
    if os.path.exists(dossier_src):
        copier_images(dossier_src, dossier_dst)

# Fusionner le dataset dataset_2
dataset2 = "dataset_2"

# Copier les images de dataset_2/train dans nouveau_dataset/train
for cat in categories:
    dossier_src = os.path.join(dataset2, "train", cat)
    dossier_dst = os.path.join(train_nouveau, cat)
    if os.path.exists(dossier_src):
        copier_images(dossier_src, dossier_dst)

# Copier les images de dataset_2/val (traité comme test) dans nouveau_dataset/test
for cat in categories:
    dossier_src = os.path.join(dataset2, "val", cat)
    dossier_dst = os.path.join(test_nouveau, cat)
    if os.path.exists(dossier_src):
        copier_images(dossier_src, dossier_dst)

print("Fusion terminée. Le nouveau dataset est disponible dans le dossier:", nouveau_dataset)

import shutil

# Nom du dossier à zipper et nom de l'archive (sans extension)
dossier = "nouveau_dataset"
nom_archive = "nouveau_dataset_archive"

# Création de l'archive au format zip
shutil.make_archive(nom_archive, 'zip', dossier)

print(f"Archive {nom_archive}.zip créée avec succès.")

